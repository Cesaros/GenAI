{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Cesaros/GenAI/blob/main/Agents_Bootcamp_Tutorial3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What is GAIA?\n",
        "\n",
        "[GAIA](https://huggingface.co/papers/2311.12983) is a **benchmark designed to evaluate AI assistants on real-world tasks** that require a combination of core capabilitiesâ€”such as reasoning, multimodal understanding, web browsing, and proficient tool use.\n",
        "\n",
        "It was introduced in the paper _\"[GAIA: A Benchmark for General AI Assistants](https://huggingface.co/papers/2311.12983)\"_.\n",
        "\n",
        "The benchmark features **466 carefully curated questions** that are **conceptually simple for humans**, yet **remarkably challenging for current AI systems**.\n",
        "\n",
        "To illustrate the gap:\n",
        "- **Humans**: ~92% success rate  \n",
        "- **GPT-4 with plugins**: ~15%  \n",
        "- **Deep Research (OpenAI)**: 67.36% on the validation set\n",
        "\n",
        "GAIA highlights the current limitations of AI models and provides a rigorous benchmark to evaluate progress toward truly general-purpose AI assistants.\n",
        "\n",
        "## ðŸŒ± GAIAâ€™s Core Principles\n",
        "\n",
        "GAIA is carefully designed around the following pillars:\n",
        "\n",
        "- ðŸ” **Real-world difficulty**: Tasks require multi-step reasoning, multimodal understanding, and tool interaction.\n",
        "- ðŸ§¾ **Human interpretability**: Despite their difficulty for AI, tasks remain conceptually simple and easy to follow for humans.\n",
        "- ðŸ›¡ï¸ **Non-gameability**: Correct answers demand full task execution, making brute-forcing ineffective.\n",
        "- ðŸ§° **Simplicity of evaluation**: Answers are concise, factual, and unambiguousâ€”ideal for benchmarking.\n",
        "\n",
        "## Difficulty Levels\n",
        "\n",
        "GAIA tasks are organized into **three levels of increasing complexity**, each testing specific skills:\n",
        "\n",
        "- **Level 1**: Requires less than 5 steps and minimal tool usage.\n",
        "- **Level 2**: Involves more complex reasoning and coordination between multiple tools and 5-10 steps.\n",
        "- **Level 3**: Demands long-term planning and advanced integration of various tools.\n",
        "\n",
        "![GAIA levels](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit4/gaia_levels.png)\n",
        "\n",
        "## Example of a Hard GAIA Question\n",
        "\n",
        "> Which of the fruits shown in the 2008 painting \"Embroidery from Uzbekistan\" were served as part of the October 1949 breakfast menu for the ocean liner that was later used as a floating prop for the film \"The Last Voyage\"? Give the items as a comma-separated list, ordering them in clockwise order based on their arrangement in the painting starting from the 12 o'clock position. Use the plural form of each fruit.\n",
        "\n",
        "As you can see, this question challenges AI systems in several ways:\n",
        "\n",
        "- Requires a **structured response format**\n",
        "- Involves **multimodal reasoning** (e.g., analyzing images)\n",
        "- Demands **multi-hop retrieval** of interdependent facts:\n",
        "  - Identifying the fruits in the painting\n",
        "  - Discovering which ocean liner was used in *The Last Voyage*\n",
        "  - Looking up the breakfast menu from October 1949 for that ship\n",
        "- Needs **correct sequencing** and high-level planning to solve in the right order\n",
        "\n",
        "This kind of task highlights where standalone LLMs often fall short, making GAIA an ideal benchmark for **agent-based systems** that can reason, retrieve, and execute over multiple steps and modalities.\n",
        "\n",
        "![GAIA capabilities plot](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit4/gaia_capabilities.png)\n",
        "\n",
        "## Live Evaluation\n",
        "\n",
        "To encourage continuous benchmarking, **GAIA provides a public leaderboard hosted on Hugging Face**, where you can test your models against **300 testing questions**.\n",
        "\n",
        "ðŸ‘‰ Check out the leaderboard [here](https://huggingface.co/spaces/gaia-benchmark/leaderboard)\n",
        "\n",
        "<iframe\n",
        "\tsrc=\"https://gaia-benchmark-leaderboard.hf.space\"\n",
        "\tframeborder=\"0\"\n",
        "\twidth=\"850\"\n",
        "\theight=\"450\"\n",
        "></iframe>"
      ],
      "metadata": {
        "id": "i6y3rwr9oFho"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "WjqMbKEZnevN",
        "outputId": "449cd3e4-08f4-4f5a-bed8-01168258460f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.lib.display.IFrame at 0x78e319d8a490>"
            ],
            "text/html": [
              "\n",
              "        <iframe\n",
              "            width=\"1000\"\n",
              "            height=\"500\"\n",
              "            src=\"https://gaia-benchmark-leaderboard.hf.space\"\n",
              "            frameborder=\"0\"\n",
              "            allowfullscreen\n",
              "            \n",
              "        ></iframe>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "from IPython.display import IFrame\n",
        "IFrame(src=\"https://gaia-benchmark-leaderboard.hf.space\", width=1000, height=500)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Our Dataset\n",
        "\n",
        "The Dataset used in this leaderboard consist of 20 questions extracted from the level 1 questions of the **validation** set from GAIA.\n",
        "\n",
        "The chosen question were filtered based on the number of tools and steps needed to answer a question.\n",
        "\n",
        "Based on the current look of the GAIA benchmark, we think that getting you to try to aim for 30% on level 1 question is a fair test.\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit4/leaderboard%20GAIA%2024%3A04%3A2025.png\" alt=\"GAIA current status!\" />\n",
        "\n",
        "## The process\n",
        "\n",
        "Now the big question in your mind is probably : \"How do I start submitting ?\"\n",
        "\n",
        "For this, we created an API that will allow you to get the questions, and send your answers for scoring.\n",
        "Here is a summary of the routes.\n",
        "\n",
        "* **`GET /questions`**: Retrieve the full list of filtered evaluation questions.\n",
        "* **`GET /random-question`**: Fetch a single random question from the list.\n",
        "* **`GET /files/{task_id}`**: Download a specific file associated with a given task ID.\n",
        "* **`POST /submit`**: Submit agent answers, calculate the score, and update the leaderboard.\n",
        "\n",
        "The submit function will compare the answer to the ground truth in an **EXACT MATCH** manner, hence prompt it well !\n",
        "\n",
        "The GAIA team shared a prompting example for your agent [here](https://huggingface.co/spaces/gaia-benchmark/leaderboard) (for the sake of this course, make sure you don't include the text \"FINAL ANSWER\" in your submission, just make your agent reply with the answer and nothing else)."
      ],
      "metadata": {
        "id": "nfm8e-cGrlTT"
      }
    }
  ]
}